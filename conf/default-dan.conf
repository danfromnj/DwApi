{
    server {
        max_workers: 2
    }

    spark {
        home: ${SPARK_HOME}
        paths: [${SPARK_HOME}/python,
                ${SPARK_HOME}/bin,
                ${SPARK_HOME}/python/lib/py4j-0.10.3-src.zip]
        app_name: dwapi_dev
        master: yarn-client
        repartition: True
        part_num: 16
        conf: [{k: spark.scheduler.mode, v: FAIR},
               {k: spark.sql.shuffle.partitions, v: 4},
               {k: spark.io.compression.codec, v: lzf},
               {k: spark.speculation, v: true},
               {k: spark.yarn.am.cores, v: 1},
               {k: spark.yarn.am.memory, v: 1g}
        ]
    }

    data_source: []

    api {
        default {
            limit: 10
        }
        cache {
            enabled: True
            expire: 604800 # unit seconds for one week
        }
        allowed: [cis-developer, cis-daily, cis-staging, cis-production, cis-qa]
    }

    scheduler {
        max_workers: 20
        max_instances: 10
        # interval unit: minutes
        interval: 1440
    }

    redis {
        host: nkg-hdp-slave-01.citrite.net
        port: 6379
        db: 0
    }

    database {
        engine: "postgresql://postgres:@nkg-postgres-01.citrite.net:5432/dwapi_nkgcluster"
        echo: False
    }

    secret = "gp9tcm7dZ+X#r5zTbNxX#cXhp2gJhL4zq$fXvUAUA2pS5$8*!a$YZuWfVJQB6Y+-VLUp+Y_wC%T47x8w+&M#NXes7F_$cnUGyxxNDS!=4@zMGQg3LcM%PA%xvjq38Cg6"
}